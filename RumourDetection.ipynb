{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer \n",
    "from textblob import TextBlob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caption</th>\n",
       "      <th>Total_reactions</th>\n",
       "      <th>No_of_Comment</th>\n",
       "      <th>Share</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Reaction</th>\n",
       "      <th>Post_url</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Like</th>\n",
       "      <th>Haha</th>\n",
       "      <th>Love</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Wow</th>\n",
       "      <th>Angry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald J. Trump's debate study habits, Hillary...</td>\n",
       "      <td>58</td>\n",
       "      <td>103</td>\n",
       "      <td>8</td>\n",
       "      <td>Name                    ...</td>\n",
       "      <td>['51 Like', '3 Haha', '3 Angry']</td>\n",
       "      <td>https://www.facebook.com/cnnpolitics/posts/128...</td>\n",
       "      <td>mostly true</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since 1956, no presidential candidate has won ...</td>\n",
       "      <td>33</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>Name  \\\\n0   Francisc...</td>\n",
       "      <td>['31 Like', '2 Love']</td>\n",
       "      <td>https://www.facebook.com/cnnpolitics/posts/128...</td>\n",
       "      <td>mostly true</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Only when his handlers tied him down and forc...</td>\n",
       "      <td>497</td>\n",
       "      <td>383</td>\n",
       "      <td>111</td>\n",
       "      <td>Name                     ...</td>\n",
       "      <td>['417 Like', '42 Love', '21 Angry']</td>\n",
       "      <td>https://www.facebook.com/cnnpolitics/posts/128...</td>\n",
       "      <td>mostly true</td>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A manufactured storyline. A promise of a \"majo...</td>\n",
       "      <td>81</td>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>Name                     ...</td>\n",
       "      <td>['59 Like', '12 Angry', '10 Haha']</td>\n",
       "      <td>https://www.facebook.com/cnnpolitics/posts/128...</td>\n",
       "      <td>mostly true</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"There's a reason why we haven't had a woman p...</td>\n",
       "      <td>513</td>\n",
       "      <td>331</td>\n",
       "      <td>116</td>\n",
       "      <td>Name                     ...</td>\n",
       "      <td>['462 Like', '25 Love', '12 Haha']</td>\n",
       "      <td>https://www.facebook.com/cnnpolitics/posts/128...</td>\n",
       "      <td>mostly true</td>\n",
       "      <td>462</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Caption  Total_reactions  \\\n",
       "0  Donald J. Trump's debate study habits, Hillary...               58   \n",
       "1  Since 1956, no presidential candidate has won ...               33   \n",
       "2  \"Only when his handlers tied him down and forc...              497   \n",
       "3  A manufactured storyline. A promise of a \"majo...               81   \n",
       "4  \"There's a reason why we haven't had a woman p...              513   \n",
       "\n",
       "   No_of_Comment  Share                                            Comment  \\\n",
       "0            103      8                        Name                    ...   \n",
       "1             64      6                           Name  \\\\n0   Francisc...   \n",
       "2            383    111                       Name                     ...   \n",
       "3            123     17                       Name                     ...   \n",
       "4            331    116                       Name                     ...   \n",
       "\n",
       "                              Reaction  \\\n",
       "0     ['51 Like', '3 Haha', '3 Angry']   \n",
       "1                ['31 Like', '2 Love']   \n",
       "2  ['417 Like', '42 Love', '21 Angry']   \n",
       "3   ['59 Like', '12 Angry', '10 Haha']   \n",
       "4   ['462 Like', '25 Love', '12 Haha']   \n",
       "\n",
       "                                            Post_url       Rating  Like  Haha  \\\n",
       "0  https://www.facebook.com/cnnpolitics/posts/128...  mostly true    51     3   \n",
       "1  https://www.facebook.com/cnnpolitics/posts/128...  mostly true    31     0   \n",
       "2  https://www.facebook.com/cnnpolitics/posts/128...  mostly true   417     0   \n",
       "3  https://www.facebook.com/cnnpolitics/posts/128...  mostly true    59    10   \n",
       "4  https://www.facebook.com/cnnpolitics/posts/128...  mostly true   462    12   \n",
       "\n",
       "   Love  Sad  Wow  Angry  \n",
       "0     0    0    0      3  \n",
       "1     2    0    0      0  \n",
       "2    42    0    0     21  \n",
       "3     0    0    0     12  \n",
       "4    25    0    0      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('f_dataf.csv')\n",
    "\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name comment jonathan del carmen jonathan del carmen hillary clinton lead jack schroth jack schroth strategy bring mu lisa bivona lisa bivona melinda maynard hyatt lol suzie sevigny suzie sevigny time america anthony colÃ³n anthony colÃ³n gary johnson american busi jonathan del carmen jonathan del carmen hillary clinton still lead mickey shay mickey shay here debate strategynletgary delina hess blanchett delina hess blanchett medium atten darrell blaser darrell blaser banned documentary episode dave anderson dave anderson cnn anti america digusting titi oshodi titi oshodi get education stop repeating lie billy master billy master httplfacebookcomlphp john sparacio john sparacio plan call total amnest darrell blaser darrell blaser black man trump shirt confro melinda hyatt melinda hyatt wikileaks confirms hillary sol chad smith chad smith cnn protrump ashley mulder ashley mulder practice saying yuuuuuge th rebecca laskey rebecca laskey republican businessman donald j danette ej danette ej hear train comin ðŸš‚ðŸš‚ðŸ’¨ncho pnar bradford pnar bradford trump russia connectionnhttp dearly vega dearly vega donald trump honest man qu rebecca laskey rebecca laskey hey mainstream medium abo linda walsh linda walsh hillary cant keep america safe abagail garcia smith abagail garcia smith latino american wome caryn macron caryn macron destroy jeff reddick jeff reddick carlos santana â€™ former lead voca rebecca laskey rebecca laskey hey mainstream medium abo john kelly john kelly cnn stop talking donald kc pal kc pal hillary clinton going white hou javi melendez javi melendez latino man never su doc joe villarreal doc joe villarreal expert pundit tracey wooldridge tracey wooldridge hillary cant even fill br donna sample donna sample im curious see excuse tru phillip armstrong phillip armstrong trump debate performance w paul booth paul booth httpsyoutubedmsurfdei suzanne allen crabtree suzanne allen crabtree nothing man say ad caroline talley caroline talley trump rebecca laskey rebecca laskey concerned que diana cox diana cox get withthe rebecca laskey rebecca laskey narcississtic pathological krista clark krista clark defence painful talking solÃ¡ toÃ­n solÃ¡ toÃ­n he busy watching fox friend eva kovacs eva kovacs well said mr walsh could al carol forbes carol forbes republican party seems hav adam camilleri adam camilleri well expert â€™ ev chris waldron chris waldron came area conway sc george giret george giret tim wardnand yet china economy tim ward tim ward need yuan jerry smith jr jerry smith jr someone hold tim ward tim ward â€™ actually le â€™ krassenstein brother krassenstein brother much money would mari mari mari mari sad whole u â€™ eva kovacs eva kovacs received word china mu mag smith mag smith â€™ received word th \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_t = data.iloc[:,4].values\n",
    "\n",
    "\n",
    "\n",
    "a = x_t[0].lower()\n",
    "\n",
    "\n",
    "\n",
    "a = re.sub(r'\\d+', '', a)\n",
    "\n",
    "\n",
    "\n",
    "a = a.strip()\n",
    "\n",
    "\n",
    "\n",
    "a = a.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "tokens = word_tokenize(a)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemat = []\n",
    "for word in result:\n",
    "    lemat.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "stri = \"\"\n",
    "for i in lemat:\n",
    "    stri = stri + i + \" \"\n",
    "\n",
    "\n",
    "\n",
    "print(stri)\n",
    "\n",
    "corpus = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for k in x_t:\n",
    "    a = k.lower()\n",
    "    a = re.sub(r'\\d+', '', a)\n",
    "    a = a.strip()\n",
    "    a = a.translate(str.maketrans('','', string.punctuation))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(a)\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    lemat = []\n",
    "    for word in result:\n",
    "        lemat.append(lemmatizer.lemmatize(word))\n",
    "    stri = \"\"\n",
    "    for i in lemat:\n",
    "        stri = stri + i + \" \"\n",
    "    corpus.append(stri)\n",
    "\n",
    "\n",
    "\n",
    "len(corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer()\n",
    "x= tf.fit_transform(corpus).toarray()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_t = data.iloc[:,7].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "type(y_t)\n",
    "\n",
    "\n",
    "\n",
    "y_t.shape\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "y_t = y_t.tolist()\n",
    "b = Counter(y_t).keys()\n",
    "\n",
    "b\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_t = labelencoder_y.fit_transform(y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 0 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 0 2 0 2 2 2 2 2 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 2 1 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n",
      " 0 0 0 2 2 2]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "  (0, 1)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 1)\t1.0\n",
      "  (6, 1)\t1.0\n",
      "  (7, 1)\t1.0\n",
      "  (8, 1)\t1.0\n",
      "  (9, 1)\t1.0\n",
      "  (10, 2)\t1.0\n",
      "  (11, 2)\t1.0\n",
      "  (12, 2)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 2)\t1.0\n",
      "  (15, 2)\t1.0\n",
      "  (16, 2)\t1.0\n",
      "  (17, 2)\t1.0\n",
      "  (18, 2)\t1.0\n",
      "  (19, 2)\t1.0\n",
      "  (20, 2)\t1.0\n",
      "  (21, 2)\t1.0\n",
      "  (22, 2)\t1.0\n",
      "  (23, 2)\t1.0\n",
      "  (24, 0)\t1.0\n",
      "  :\t:\n",
      "  (425, 1)\t1.0\n",
      "  (426, 1)\t1.0\n",
      "  (427, 1)\t1.0\n",
      "  (428, 1)\t1.0\n",
      "  (429, 1)\t1.0\n",
      "  (430, 1)\t1.0\n",
      "  (431, 1)\t1.0\n",
      "  (432, 1)\t1.0\n",
      "  (433, 1)\t1.0\n",
      "  (434, 1)\t1.0\n",
      "  (435, 1)\t1.0\n",
      "  (436, 1)\t1.0\n",
      "  (437, 1)\t1.0\n",
      "  (438, 1)\t1.0\n",
      "  (439, 1)\t1.0\n",
      "  (440, 1)\t1.0\n",
      "  (441, 2)\t1.0\n",
      "  (442, 2)\t1.0\n",
      "  (443, 2)\t1.0\n",
      "  (444, 0)\t1.0\n",
      "  (445, 0)\t1.0\n",
      "  (446, 0)\t1.0\n",
      "  (447, 2)\t1.0\n",
      "  (448, 2)\t1.0\n",
      "  (449, 2)\t1.0\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goels\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([1.95621037, 1.74549198, 2.00658464, 1.90198493, 1.6050837 ]), 'score_time': array([0.09271073, 0.09984303, 0.08183932, 0.0885129 , 0.07477212]), 'test_score': array([0.92647059, 0.86764706, 0.85074627, 0.88059701, 0.7761194 ])}\n",
      "(337, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(y_t)\n",
    "\n",
    "\n",
    "# In[225]:\n",
    "\n",
    "\n",
    "y_t = y_t.reshape(-1,1)\n",
    "print(y_t)\n",
    "\n",
    "\n",
    "# In[226]:\n",
    "\n",
    "\n",
    "onehotencoder_y = OneHotEncoder()\n",
    "y_t = onehotencoder_y.fit_transform(y_t)\n",
    "\n",
    "\n",
    "# In[227]:\n",
    "\n",
    "\n",
    "print(y_t)\n",
    "\n",
    "\n",
    "# In[228]:\n",
    "\n",
    "\n",
    "y_t = y_t.toarray()\n",
    "y1_t= y_t\n",
    "\n",
    "\n",
    "# In[229]:\n",
    "\n",
    "\n",
    "print(y_t)\n",
    "\n",
    "\n",
    "# In[195]:\n",
    "\n",
    "\n",
    "#type(y_t)\n",
    "#import numpy as np\n",
    "#y_t = np.delete(y_t,y_t.shape[1]-1,axis=1)\n",
    "\n",
    "\n",
    "# In[196]:\n",
    "\n",
    "\n",
    "print(y_t)\n",
    "\n",
    "\n",
    "# In[230]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y_t, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "# In[231]:\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.model_selection import cross_validate\n",
    "classifier2 = LabelPowerset(SVC(kernel = 'rbf',probability=True, random_state = 0,gamma='auto'))\n",
    "classifier2.fit(X_train,y_train)\n",
    "scores = cross_validate(classifier2,X_train,y_train,cv=5)\n",
    "print(scores)\n",
    "\n",
    "\n",
    "# In[235]:\n",
    "\n",
    "\n",
    "y_pred = classifier2.predict(X_test)\n",
    "y1_pred = classifier2.predict(X_train)\n",
    "print(y1_pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 1)\t1\n",
      "  (5, 1)\t1\n",
      "  (6, 1)\t1\n",
      "  (7, 1)\t1\n",
      "  (8, 1)\t1\n",
      "  (9, 1)\t1\n",
      "  (10, 1)\t1\n",
      "  (11, 1)\t1\n",
      "  (12, 1)\t1\n",
      "  (13, 1)\t1\n",
      "  (14, 1)\t1\n",
      "  (15, 1)\t1\n",
      "  (16, 1)\t1\n",
      "  (17, 1)\t1\n",
      "  (18, 1)\t1\n",
      "  (19, 1)\t1\n",
      "  (20, 1)\t1\n",
      "  (21, 1)\t1\n",
      "  (22, 1)\t1\n",
      "  (23, 1)\t1\n",
      "  (24, 1)\t1\n",
      "  (25, 1)\t1\n",
      "  (26, 1)\t1\n",
      "  (27, 1)\t1\n",
      "  (28, 1)\t1\n",
      "  (29, 1)\t1\n",
      "  (30, 1)\t1\n",
      "  (31, 1)\t1\n",
      "  (32, 1)\t1\n",
      "  (33, 1)\t1\n",
      "  (34, 1)\t1\n",
      "  (35, 1)\t1\n",
      "  (36, 1)\t1\n",
      "  (37, 1)\t1\n",
      "  (38, 1)\t1\n",
      "  (39, 1)\t1\n",
      "  (40, 1)\t1\n",
      "  (41, 1)\t1\n",
      "  (42, 1)\t1\n",
      "  (43, 1)\t1\n",
      "  (44, 1)\t1\n",
      "  (45, 1)\t1\n",
      "  (46, 1)\t1\n",
      "  (47, 1)\t1\n",
      "  (48, 1)\t1\n",
      "  (49, 1)\t1\n",
      "  (50, 1)\t1\n",
      "  (51, 1)\t1\n",
      "  (52, 1)\t1\n",
      "  (53, 1)\t1\n",
      "  (54, 1)\t1\n",
      "  (55, 1)\t1\n",
      "  (56, 1)\t1\n",
      "  (57, 1)\t1\n",
      "  (58, 1)\t1\n",
      "  (59, 1)\t1\n",
      "  (60, 1)\t1\n",
      "  (61, 1)\t1\n",
      "  (62, 1)\t1\n",
      "  (63, 1)\t1\n",
      "  (64, 1)\t1\n",
      "  (65, 1)\t1\n",
      "  (66, 1)\t1\n",
      "  (67, 1)\t1\n",
      "  (68, 1)\t1\n",
      "  (69, 1)\t1\n",
      "  (70, 1)\t1\n",
      "  (71, 1)\t1\n",
      "  (72, 1)\t1\n",
      "  (73, 1)\t1\n",
      "  (74, 1)\t1\n",
      "  (75, 1)\t1\n",
      "  (76, 1)\t1\n",
      "  (77, 1)\t1\n",
      "  (78, 1)\t1\n",
      "  (79, 1)\t1\n",
      "  (80, 1)\t1\n",
      "  (81, 1)\t1\n",
      "  (82, 1)\t1\n",
      "  (83, 1)\t1\n",
      "  (84, 1)\t1\n",
      "  (85, 1)\t1\n",
      "  (86, 1)\t1\n",
      "  (87, 1)\t1\n",
      "  (88, 1)\t1\n",
      "  (89, 1)\t1\n",
      "  (90, 1)\t1\n",
      "  (91, 1)\t1\n",
      "  (92, 1)\t1\n",
      "  (93, 1)\t1\n",
      "  (94, 1)\t1\n",
      "  (95, 1)\t1\n",
      "  (96, 1)\t1\n",
      "  (97, 1)\t1\n",
      "  (98, 1)\t1\n",
      "  (99, 1)\t1\n",
      "  (100, 1)\t1\n",
      "  (101, 1)\t1\n",
      "  (102, 1)\t1\n",
      "  (103, 1)\t1\n",
      "  (104, 1)\t1\n",
      "  (105, 1)\t1\n",
      "  (106, 1)\t1\n",
      "  (107, 1)\t1\n",
      "  (108, 1)\t1\n",
      "  (109, 1)\t1\n",
      "  (110, 1)\t1\n",
      "  (111, 1)\t1\n",
      "  (112, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8672566371681416, 0.8672566371681416, 0.8672566371681416)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(y_pred[99])\n",
    "\n",
    "\n",
    "# In[246]:\n",
    "\n",
    "\n",
    "a=y_pred.toarray()\n",
    "\n",
    "b= y1_pred.toarray()\n",
    "# In[247]:\n",
    "\n",
    "\n",
    "a\n",
    "\n",
    "\n",
    "# In[248]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "\n",
    "\n",
    "# In[249]:\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# In[250]:\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "# In[251]:\n",
    "\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "# In[252]:\n",
    "\n",
    "\n",
    "acc,precision,recall\n",
    "\n",
    "\n",
    "# In[254]:\n",
    "\n",
    "#y = dataset.iloc[:, 8].values\n",
    "\n",
    "\n",
    "# In[255]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in b:\n",
    "    l.append(i)\n",
    "for i in a:\n",
    "    l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(l[0])[0]\n",
    "import numpy as np\n",
    "f = np.empty([450,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,450):\n",
    "    for j in range(0,3):\n",
    "        f[i][j] = (l[i])[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[29][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "  \n",
    "# Save the trained model as a pickle string. \n",
    "saved_model = pickle.dumps(classifier2) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'column = [2,3,4,9,10,11,12,13,14,15,16,17]\\nX1 = dataset.iloc[:, column].values\\nynn = dataset.iloc[:, 8].values'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('f_dataf.csv')\n",
    "\"\"\"column = [2,3,4,9,10,11,12,13,14,15,16,17]\n",
    "X1 = dataset.iloc[:, column].values\n",
    "ynn = dataset.iloc[:, 8].values\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1=[]\n",
    "svm2=[]\n",
    "svm3=[]\n",
    "for i in range(0,450):\n",
    "    svm1.append(f[i][0])\n",
    "    svm2.append(f[i][1])\n",
    "    svm3.append(f[i][2])\n",
    "    \n",
    "# len(svm3)\n",
    "dataset['svm1']=svm1\n",
    "dataset['svm2']=svm2\n",
    "dataset['svm3']=svm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = [2,3,4,9,10,11,12,13,14,15,16,17]\n",
    "X1 = dataset.iloc[:, column].values\n",
    "ynn = dataset.iloc[:, 8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,400):\n",
    "    if(float(X1[i][0])!= 0):\n",
    "        (X1[i][3])=float(X1[i][3])/float(X1[i][0])\n",
    "        (X1[i][4])=float(X1[i][4])/float(X1[i][0])\n",
    "        (X1[i][5])=float(X1[i][5])/float(X1[i][0])\n",
    "        (X1[i][6])=float(X1[i][6])/float(X1[i][0])\n",
    "        (X1[i][7])=float(X1[i][7])/float(X1[i][0])\n",
    "        (X1[i][8])=float(X1[i][8])/float(X1[i][0])\n",
    "        # np.append(X1[i],[f[i][0],f[i][1],f[i][2]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.80e+01, 1.03e+02, 8.00e+00, ..., 0.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [3.30e+01, 6.40e+01, 6.00e+00, ..., 0.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [4.97e+02, 3.83e+02, 1.11e+02, ..., 0.00e+00, 1.00e+00, 0.00e+00],\n",
       "       ...,\n",
       "       [4.00e+03, 3.40e+02, 0.00e+00, ..., 0.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [3.80e+04, 1.50e+03, 0.00e+00, ..., 0.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [3.50e+04, 1.60e+03, 0.00e+00, ..., 0.00e+00, 1.00e+00, 0.00e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,400):\n",
    "#     X2.append([X1[i][0],X1[i][1],X1[i][2],X1[i][3],X1[i][4],X1[i][5],X1[i][6],X1[i][7],X1[i][8],f[i][0],f[i][1],f[i][2]])\n",
    "#X3=pd.DataFrame(X2,columns=[0,1,2,3,4,5,6,7,8,9,10,11]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X2.remove([])\n",
    "#print(X2.tolist())\n",
    "# X3 = np.array(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X4=X3[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X4[399]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y_t, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "d_train, d_test, y_train, y_test = train_test_split(X1, y_t, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.04841089, 0.04562855, 0.04944372, 0.04394102, 0.04844475]), 'score_time': array([0.00801539, 0.00399518, 0.0039947 , 0.00799108, 0.00399518]), 'test_score': array([0.92647059, 0.86764706, 0.85074627, 0.88059701, 0.7761194 ])}\n",
      "  (0, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 2)\t1\n",
      "  (5, 1)\t1\n",
      "  (6, 1)\t1\n",
      "  (7, 1)\t1\n",
      "  (8, 1)\t1\n",
      "  (9, 1)\t1\n",
      "  (10, 1)\t1\n",
      "  (11, 1)\t1\n",
      "  (12, 1)\t1\n",
      "  (13, 1)\t1\n",
      "  (14, 1)\t1\n",
      "  (15, 1)\t1\n",
      "  (16, 1)\t1\n",
      "  (17, 1)\t1\n",
      "  (18, 1)\t1\n",
      "  (19, 1)\t1\n",
      "  (20, 1)\t1\n",
      "  (21, 1)\t1\n",
      "  (22, 1)\t1\n",
      "  (23, 1)\t1\n",
      "  (24, 1)\t1\n",
      "  (25, 1)\t1\n",
      "  (26, 1)\t1\n",
      "  (27, 1)\t1\n",
      "  (28, 1)\t1\n",
      "  (29, 1)\t1\n",
      "  (30, 1)\t1\n",
      "  (31, 1)\t1\n",
      "  (32, 1)\t1\n",
      "  (33, 1)\t1\n",
      "  (34, 1)\t1\n",
      "  (35, 1)\t1\n",
      "  (36, 1)\t1\n",
      "  (37, 1)\t1\n",
      "  (38, 1)\t1\n",
      "  (39, 1)\t1\n",
      "  (40, 1)\t1\n",
      "  (41, 1)\t1\n",
      "  (42, 1)\t1\n",
      "  (43, 2)\t1\n",
      "  (44, 1)\t1\n",
      "  (45, 1)\t1\n",
      "  (46, 1)\t1\n",
      "  (47, 1)\t1\n",
      "  (48, 1)\t1\n",
      "  (49, 1)\t1\n",
      "  (50, 1)\t1\n",
      "  (51, 1)\t1\n",
      "  (52, 1)\t1\n",
      "  (53, 1)\t1\n",
      "  (54, 1)\t1\n",
      "  (55, 1)\t1\n",
      "  (56, 1)\t1\n",
      "  (57, 1)\t1\n",
      "  (58, 2)\t1\n",
      "  (59, 1)\t1\n",
      "  (60, 1)\t1\n",
      "  (61, 1)\t1\n",
      "  (62, 2)\t1\n",
      "  (63, 1)\t1\n",
      "  (64, 1)\t1\n",
      "  (65, 1)\t1\n",
      "  (66, 1)\t1\n",
      "  (67, 1)\t1\n",
      "  (68, 1)\t1\n",
      "  (69, 1)\t1\n",
      "  (70, 1)\t1\n",
      "  (71, 1)\t1\n",
      "  (72, 1)\t1\n",
      "  (73, 1)\t1\n",
      "  (74, 1)\t1\n",
      "  (75, 1)\t1\n",
      "  (76, 1)\t1\n",
      "  (77, 1)\t1\n",
      "  (78, 1)\t1\n",
      "  (79, 1)\t1\n",
      "  (80, 1)\t1\n",
      "  (81, 1)\t1\n",
      "  (82, 2)\t1\n",
      "  (83, 1)\t1\n",
      "  (84, 1)\t1\n",
      "  (85, 1)\t1\n",
      "  (86, 1)\t1\n",
      "  (87, 2)\t1\n",
      "  (88, 1)\t1\n",
      "  (89, 1)\t1\n",
      "  (90, 2)\t1\n",
      "  (91, 2)\t1\n",
      "  (92, 1)\t1\n",
      "  (93, 1)\t1\n",
      "  (94, 1)\t1\n",
      "  (95, 1)\t1\n",
      "  (96, 1)\t1\n",
      "  (97, 1)\t1\n",
      "  (98, 1)\t1\n",
      "  (99, 1)\t1\n",
      "  (100, 2)\t1\n",
      "  (101, 1)\t1\n",
      "  (102, 1)\t1\n",
      "  (103, 1)\t1\n",
      "  (104, 1)\t1\n",
      "  (105, 1)\t1\n",
      "  (106, 2)\t1\n",
      "  (107, 1)\t1\n",
      "  (108, 1)\t1\n",
      "  (109, 1)\t1\n",
      "  (110, 1)\t1\n",
      "  (111, 1)\t1\n",
      "  (112, 1)\t1\n",
      "  (113, 2)\t1\n",
      "  (114, 1)\t1\n",
      "  (115, 0)\t1\n",
      "  (116, 1)\t1\n",
      "  (117, 1)\t1\n",
      "  (118, 1)\t1\n",
      "  (119, 1)\t1\n",
      "  (120, 1)\t1\n",
      "  (121, 1)\t1\n",
      "  (122, 1)\t1\n",
      "  (123, 1)\t1\n",
      "  (124, 2)\t1\n",
      "  (125, 1)\t1\n",
      "  (126, 1)\t1\n",
      "  (127, 1)\t1\n",
      "  (128, 1)\t1\n",
      "  (129, 1)\t1\n",
      "  (130, 1)\t1\n",
      "  (131, 1)\t1\n",
      "  (132, 1)\t1\n",
      "  (133, 1)\t1\n",
      "  (134, 1)\t1\n",
      "  (135, 1)\t1\n",
      "  (136, 1)\t1\n",
      "  (137, 1)\t1\n",
      "  (138, 1)\t1\n",
      "  (139, 1)\t1\n",
      "  (140, 1)\t1\n",
      "  (141, 1)\t1\n",
      "  (142, 1)\t1\n",
      "  (143, 1)\t1\n",
      "  (144, 1)\t1\n",
      "  (145, 1)\t1\n",
      "  (146, 1)\t1\n",
      "  (147, 1)\t1\n",
      "  (148, 0)\t1\n",
      "  (149, 1)\t1\n",
      "  (150, 1)\t1\n",
      "  (151, 0)\t1\n",
      "  (152, 1)\t1\n",
      "  (153, 2)\t1\n",
      "  (154, 1)\t1\n",
      "  (155, 2)\t1\n",
      "  (156, 0)\t1\n",
      "  (157, 1)\t1\n",
      "  (158, 1)\t1\n",
      "  (159, 1)\t1\n",
      "  (160, 0)\t1\n",
      "  (161, 1)\t1\n",
      "  (162, 0)\t1\n",
      "  (163, 1)\t1\n",
      "  (164, 1)\t1\n",
      "  (165, 1)\t1\n",
      "  (166, 2)\t1\n",
      "  (167, 1)\t1\n",
      "  (168, 1)\t1\n",
      "  (169, 1)\t1\n",
      "  (170, 1)\t1\n",
      "  (171, 2)\t1\n",
      "  (172, 1)\t1\n",
      "  (173, 1)\t1\n",
      "  (174, 1)\t1\n",
      "  (175, 1)\t1\n",
      "  (176, 1)\t1\n",
      "  (177, 1)\t1\n",
      "  (178, 1)\t1\n",
      "  (179, 1)\t1\n",
      "  (180, 1)\t1\n",
      "  (181, 1)\t1\n",
      "  (182, 2)\t1\n",
      "  (183, 1)\t1\n",
      "  (184, 1)\t1\n",
      "  (185, 1)\t1\n",
      "  (186, 1)\t1\n",
      "  (187, 1)\t1\n",
      "  (188, 1)\t1\n",
      "  (189, 1)\t1\n",
      "  (190, 1)\t1\n",
      "  (191, 1)\t1\n",
      "  (192, 1)\t1\n",
      "  (193, 1)\t1\n",
      "  (194, 1)\t1\n",
      "  (195, 1)\t1\n",
      "  (196, 1)\t1\n",
      "  (197, 1)\t1\n",
      "  (198, 1)\t1\n",
      "  (199, 1)\t1\n",
      "  (200, 1)\t1\n",
      "  (201, 1)\t1\n",
      "  (202, 1)\t1\n",
      "  (203, 2)\t1\n",
      "  (204, 1)\t1\n",
      "  (205, 1)\t1\n",
      "  (206, 1)\t1\n",
      "  (207, 2)\t1\n",
      "  (208, 1)\t1\n",
      "  (209, 1)\t1\n",
      "  (210, 0)\t1\n",
      "  (211, 1)\t1\n",
      "  (212, 2)\t1\n",
      "  (213, 1)\t1\n",
      "  (214, 1)\t1\n",
      "  (215, 1)\t1\n",
      "  (216, 1)\t1\n",
      "  (217, 1)\t1\n",
      "  (218, 1)\t1\n",
      "  (219, 1)\t1\n",
      "  (220, 1)\t1\n",
      "  (221, 1)\t1\n",
      "  (222, 1)\t1\n",
      "  (223, 2)\t1\n",
      "  (224, 0)\t1\n",
      "  (225, 1)\t1\n",
      "  (226, 1)\t1\n",
      "  (227, 1)\t1\n",
      "  (228, 1)\t1\n",
      "  (229, 1)\t1\n",
      "  (230, 1)\t1\n",
      "  (231, 1)\t1\n",
      "  (232, 1)\t1\n",
      "  (233, 1)\t1\n",
      "  (234, 1)\t1\n",
      "  (235, 1)\t1\n",
      "  (236, 1)\t1\n",
      "  (237, 1)\t1\n",
      "  (238, 1)\t1\n",
      "  (239, 1)\t1\n",
      "  (240, 2)\t1\n",
      "  (241, 1)\t1\n",
      "  (242, 1)\t1\n",
      "  (243, 1)\t1\n",
      "  (244, 1)\t1\n",
      "  (245, 1)\t1\n",
      "  (246, 1)\t1\n",
      "  (247, 1)\t1\n",
      "  (248, 1)\t1\n",
      "  (249, 1)\t1\n",
      "  (250, 1)\t1\n",
      "  (251, 1)\t1\n",
      "  (252, 2)\t1\n",
      "  (253, 1)\t1\n",
      "  (254, 1)\t1\n",
      "  (255, 1)\t1\n",
      "  (256, 1)\t1\n",
      "  (257, 1)\t1\n",
      "  (258, 1)\t1\n",
      "  (259, 1)\t1\n",
      "  (260, 1)\t1\n",
      "  (261, 1)\t1\n",
      "  (262, 1)\t1\n",
      "  (263, 1)\t1\n",
      "  (264, 1)\t1\n",
      "  (265, 1)\t1\n",
      "  (266, 1)\t1\n",
      "  (267, 1)\t1\n",
      "  (268, 1)\t1\n",
      "  (269, 1)\t1\n",
      "  (270, 1)\t1\n",
      "  (271, 2)\t1\n",
      "  (272, 1)\t1\n",
      "  (273, 1)\t1\n",
      "  (274, 1)\t1\n",
      "  (275, 0)\t1\n",
      "  (276, 2)\t1\n",
      "  (277, 1)\t1\n",
      "  (278, 1)\t1\n",
      "  (279, 1)\t1\n",
      "  (280, 2)\t1\n",
      "  (281, 1)\t1\n",
      "  (282, 2)\t1\n",
      "  (283, 1)\t1\n",
      "  (284, 1)\t1\n",
      "  (285, 1)\t1\n",
      "  (286, 1)\t1\n",
      "  (287, 1)\t1\n",
      "  (288, 1)\t1\n",
      "  (289, 2)\t1\n",
      "  (290, 2)\t1\n",
      "  (291, 1)\t1\n",
      "  (292, 1)\t1\n",
      "  (293, 1)\t1\n",
      "  (294, 1)\t1\n",
      "  (295, 1)\t1\n",
      "  (296, 1)\t1\n",
      "  (297, 1)\t1\n",
      "  (298, 0)\t1\n",
      "  (299, 1)\t1\n",
      "  (300, 1)\t1\n",
      "  (301, 1)\t1\n",
      "  (302, 1)\t1\n",
      "  (303, 1)\t1\n",
      "  (304, 1)\t1\n",
      "  (305, 1)\t1\n",
      "  (306, 1)\t1\n",
      "  (307, 1)\t1\n",
      "  (308, 0)\t1\n",
      "  (309, 1)\t1\n",
      "  (310, 1)\t1\n",
      "  (311, 2)\t1\n",
      "  (312, 1)\t1\n",
      "  (313, 2)\t1\n",
      "  (314, 1)\t1\n",
      "  (315, 1)\t1\n",
      "  (316, 1)\t1\n",
      "  (317, 2)\t1\n",
      "  (318, 1)\t1\n",
      "  (319, 1)\t1\n",
      "  (320, 0)\t1\n",
      "  (321, 0)\t1\n",
      "  (322, 1)\t1\n",
      "  (323, 1)\t1\n",
      "  (324, 1)\t1\n",
      "  (325, 1)\t1\n",
      "  (326, 1)\t1\n",
      "  (327, 1)\t1\n",
      "  (328, 1)\t1\n",
      "  (329, 1)\t1\n",
      "  (330, 1)\t1\n",
      "  (331, 1)\t1\n",
      "  (332, 1)\t1\n",
      "  (333, 1)\t1\n",
      "  (334, 1)\t1\n",
      "  (335, 2)\t1\n",
      "  (336, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.model_selection import cross_validate\n",
    "classifier3 = LabelPowerset(SVC(kernel = 'rbf',probability=True, random_state = 0,gamma='auto'))\n",
    "classifier3.fit(d_train,y_train)\n",
    "scores = cross_validate(classifier3,d_train,y_train,cv=5)\n",
    "print(scores)\n",
    "\n",
    "\n",
    "# In[235]:\n",
    "\n",
    "\n",
    "y_pred = classifier3.predict(d_test)\n",
    "y1_pred = classifier3.predict(d_train)\n",
    "print(y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8761061946902655, 0.8761061946902655, 0.8761061946902655)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "\n",
    "\n",
    "# In[249]:\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# In[250]:\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "# In[251]:\n",
    "\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "# In[252]:\n",
    "\n",
    "\n",
    "acc,precision,recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "ynn= label_encoder.fit_transform(ynn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "ynn = to_categorical(ynn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "d_train, d_test, ynn_train, ynn_test = train_test_split(X1, ynn, test_size=0.25, random_state=0)\n",
    "\n",
    "print(d_train.shape)\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(units = 7, kernel_initializer = 'glorot_uniform', activation = 'relu', input_dim = 12))\n",
    "\n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 3, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    #classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fitting the ANN to the Training set\n",
    "    classifier.fit(d_train, ynn_train, batch_size = 10, epochs = 100)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9164884030818939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#print(d_train.shape)\n",
    "#kfold = KFold(n_splits=10, shuffle=True)\n",
    "classifier = KerasClassifier(build_fn=model,batch_size = 5, epochs = 150)\n",
    "results = cross_val_score(estimator=classifier, X=d_train, y=ynn_train, cv=10,n_jobs=-1)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "  \n",
    "# Save the trained model as a pickle string. \n",
    "saved_model = pickle.dumps(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
